// https://www.youtube.com/watch?v=odpjk7_tGY0

간단히 머신러닝 정의
1. Supervised Learning
    - Labeled Data
    - Direct Feedback
2. UnSupervised Learning
    - No labeled Data
    - No feedback
    - "Find hidden Structure"
3. Reinforcement Learning
    - No labeled Data
    - Delayed feedback
    - Reward signal

Supervised Learning
- 학습 하는 모델을 Discrinative model 이라고 부름
- 예를 들자면, 남자, 여자 사진을 주면 학습함

Unsupervised Learning
- Generative model learns the distribution of training data
- 어떠한 랜덤한 latent coding을 줬을때, 그 코드로 부터 Training data 예를 들자면 image를 만들어 낸다

확률 분포
예 주사위
X    =   1   2   3   4   5   6
p(x) =  1/6 1/6 1/6  0  1/6  2/6
이런식으로
만약 x 를 차원을 늘려서 이미지 차원을 학습하게 하면 어떨까?
이때도 이제 이미지 분포를 우리가 좀 예측할 수 있을까? 생각해 볼 필요 있음
그래서 여기서 말하는 x 는 이미지 64 * 64 * 3 짜리 고차원 백터로 생각할 수 있고, 저기 value 는 pixel 이라 생각하면 됨

예를 들자면 지금 여기 글에는 안보이지만 그래프가 있다 상상
y axis = probability density
x 는 음 사람 이미지 특성
Graph line 자체는 3개의 맥시멈이 있는 polynomial graph

만약 안경을 쓴 사람이 적게 나온다면 (x1),
x1의 높이 (prob density) 가 제일 낮다,

그다음 흑발의 여자가 학습데이터에서 많이 나왔다 (x2),
확률 밀도 더 높음

금발의 백인 여자가 제일 많이 나왔다 (x3),
확률 밀도 젤 높음

vector를 여기서는 2차원으로 했지만, 원래는 더 고차원임 (64,64,3) 예를 들면 image
------------------------
그러면 여기서 Supervised model 로 Training Dataset을 학습한다음
Generative model로 Training set와 유사한 data set 생성함

Generative model중 요즘 핫한 GAN
------------------------------------------------------------------------------
GAN
두가지 모델 있음
1. Discriminative model (진짜 데이터를 이용한 학습)
2. Generative model (가짜 데이터 (생선된 데이터) 를 이용한 학습)

우리가 원하는건 2번 방법 generative model 을 이용해서 학습을 하고자 하는데
그전에 1) 방법을 이용해서 학습을 함.

step 1) 진짜 데이터로 학습함
        -> 진짜 이미지로 구별 하게 학습함
step 2) 가짜 이미지로 학습
        -> 가짜 이미지가 들어갔을 때 discriminator 1번 모델이 가짜를 가짜라고 구별 할 수 있게 학습하기

그래서 input 같은 경우에는 (이미지인 경우에) 3차원 64 * 64 * 3 진짜 이미지와 가짜 이미지
그리고 output 같은 경우에는 진짜와 가짜 (binary) 하게만 구별해 내면 되니깐 일차원 적인 (0,1)로 나옴 (0 -> 가짜 1-> 진짜)
-> 보통 sigmoid 함수를 거쳐서 0.5를 기준으로 분류를 많이 합니다.
-> 그래서 가짜를 분류해야함

반대로
Generator(G) 같은 경우 random한 코드를 받아서 이미지를 생성해 내야 하는데,
생성한 걸로 Discriminator(D)를 속여야 함. -> 1이 나오도록
그래서 G 같은 경우는 학습을 하면 할수록 진짜 같은 가짜 이미지를 만들 수 있음.

------------------------------------------------------------------------
수식 적 접근 -> https://hyeongminlee.github.io/post/gan001_gan/
아직 안읽어 봤지만 흟어본 봐로는 괜찮은듯
--------------------------------------------------------------------
수식 적 접근
min(G)max(D) V(D,G) =          Ex~pdata(x) [logD(x)]    +  Ez~p(z)[log(1-D(G(z)))]
D should maximize V(D,G)
자 이게 중요한거 같은데 어려움
내가 살짝 1차적으로 이해한거는
일단 여기서 확률적 분포가 굉장히 중요하다는 것임

두개의 공식
Ex~pdata(x) [logD(x)]  -> Sample x from real data distribution
    log ->
Ez~p(z)[log(1-D(G(z)))] -> Sample latent Z from Gaussian distribution
1)
10분 나중에 다시 보자

------------------------------------------------------
Code -> Pytoch를 사용한
Step 1) D 정의 하기
i
---------------------------------------------------------
DCGAN
-> Use Convolution, Leaky ReLU
-> No pooling layer (Instead strided convolution)
-> Use batch Normalization
-> Adam optimizer(Ir= 0.0002, beta1 = 0.5, beta2 = 0.999)

arithmetic 이 가능
예를 들면
안경을 쓴남자 - 남자 = 안경
+ 여자
= 안경을 쓴 여자
여기서 의미하는게 vector 가 선형적인 관계를 갖더라.
----------------------------------------------------------------------------
Least Squares GAN (LSGAN) - DCGAN 보다 잘 나옴
Proposed a GAN Model that adopts the least squares loss function for the discriminator
32분

평가는 사람이 보고 해야함
----------------------------------------------------------------
SGAN (Semi-supervised GAN) 36분
--------------------------------------------
ACGAN 39분
--------------------------------------------
CYCLEGAN (47분)