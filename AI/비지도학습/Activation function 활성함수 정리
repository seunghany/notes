시그모이드 함수는 다음과 같은 특성을 가진다.
입력 신호의 총합을 0에서 1사이의 값으로 바꿔준다.
입력 신호의 값이 커질수록(작아질수록) 뉴런의 활성화률(firing rate)이 1(작아질 경우 0)로 수렴(saturation)한다.

하지만, 위와 같은 특성 때문에 시그모이드 함수는 2가지 문제가 있다.
    1.  입력의 절대값이 크게 되면 0이나 1로 수렴하게 되는데, 이러한 뉴런들은 그래디언트를 소멸(kill) 시켜 버린다.
        그 이유는 수렴된 뉴런의 그래디언트 값은 0이기 때문에 역전파에서 0이 곱해지기 때문이다.
        따라서, 역전파가 진행됨에 따라 아래 층(layer)에는 아무것도 전달되지 않는다.
        (시그모이드의 도함수는 sigma(1-sigma) 이므로 함수의 값이 0이나 1에 가까우면 도함수의 결과가 매우 작아진다.)

    2.  원점 중심이 아니다(Not zero-centered).  따라서, 평균이 0이 아니라 0.5 이며,
        시그모이드 함수는 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높다.
        이것을 편향 이동(bias shift)이라 하며, 이러한 이유로 각 레이어를 지날 때마다 분산이 계속 커져 가장 높은 레이어에서는
        활성화 함수의 출력이 0이나 1로 수렴하게 되어 그래디언트 소실 문제가 일어나게 된다.

출처: https://excelsior-cjh.tistory.com/177 [EXCELSIOR]

2.3 ReLU (Rectified Linear Unit)
ReLU(렐루, Rectified Linear Unit)는 시그모이드 계열과는 다른 활성화 함수이며,  아래의 식과 같이 입력이 0이상이면 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.
ReLU함수는 다음과 같은 특성을 가진다.
0 이상인 곳에서는 수렴하는 구간이 없다.
단순히 입력값을 그대로 출력으로 내보내기 때문에 시그모이드 함수에 비해 계산 속도가 빠르다.
sigmoid/tanh에 비해 stochastic gradient descent(SGD)에서 수렴속도가 무려 6배나 빠르다고 한다(Krizhevsky et al.).

출처: https://excelsior-cjh.tistory.com/177 [EXCELSIOR]